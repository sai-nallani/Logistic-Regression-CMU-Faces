{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# # # parent, subdirs, files\n",
    "# for parent, subdirs, files in os.walk('./happy_dataset'):\n",
    "#     for file in files:\n",
    "#         if not ('_2' in file or '_4' in file):\n",
    "#             if ('happy' in file):\n",
    "#                 os.rename(os.path.join(parent, file), f'./happy_dataset/happy/{file[:file.find(\".\")]}.pgm')\n",
    "#             elif ('sad' in file):\n",
    "#                 os.rename(os.path.join(parent, file), f'./happy_dataset/sad/{file[:file.find(\".\")]}.pgm')\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from imageio.v2 import imread\n",
    "\n",
    "file_names = []\n",
    "happy_ds = []\n",
    "for parent,subdirs,files in os.walk('./happy_dataset/happy'):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(parent, file)\n",
    "        img_data = imread(file_path)\n",
    "        happy_ds.append(img_data)\n",
    "        file_names.append(file_path)\n",
    "    \n",
    "sad_ds = []\n",
    "for parent,subdirs,files in os.walk('./happy_dataset/sad'):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(parent, file)\n",
    "        img_data = imread(file_path)\n",
    "        sad_ds.append(img_data)\n",
    "        file_names.append(file_path)\n",
    "\n",
    "y = np.ones(len(happy_ds)).tolist()\n",
    "y.extend(np.zeros(len(sad_ds)).tolist())\n",
    "happy_ds.extend(sad_ds)\n",
    "with h5py.File(\"happyds.h5\", \"w\") as file:\n",
    "    dset = file.create_dataset(\"x_train\", data=np.array(happy_ds))\n",
    "    dset = file.create_dataset(\"y_train\", data= np.array(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: J = 0.6931471805599452\n",
      "train accuracy = 50.8%\n",
      "100: J = 1.8273954583590601\n",
      "train accuracy = 49.2%\n",
      "200: J = 1.7578291851311703\n",
      "train accuracy = 50.8%\n",
      "300: J = 1.5866040998059456\n",
      "train accuracy = 54%\n",
      "400: J = 1.4103664931156494\n",
      "train accuracy = 56.4%\n",
      "500: J = 1.2674805700009868\n",
      "train accuracy = 58.8%\n",
      "600: J = 1.154277287534191\n",
      "train accuracy = 61.6%\n",
      "700: J = 1.0589509696816852\n",
      "train accuracy = 64%\n",
      "800: J = 0.9743322184044451\n",
      "train accuracy = 64.4%\n",
      "900: J = 0.8969440571429769\n",
      "train accuracy = 64.8%\n",
      "1000: J = 0.8249709954342059\n",
      "train accuracy = 65.2%\n",
      "1100: J = 0.7573183517178228\n",
      "train accuracy = 66%\n",
      "1200: J = 0.6931127143738514\n",
      "train accuracy = 67.2%\n",
      "1300: J = 0.6313475281148886\n",
      "train accuracy = 68.8%\n",
      "1400: J = 0.5706995407401348\n",
      "train accuracy = 72.4%\n",
      "1500: J = 0.509628746056468\n",
      "train accuracy = 74.8%\n",
      "1600: J = 0.44687826109538936\n",
      "train accuracy = 76.4%\n",
      "1700: J = 0.38254287465859477\n",
      "train accuracy = 80%\n",
      "1800: J = 0.3196180542359206\n",
      "train accuracy = 84.4%\n",
      "1900: J = 0.2634398203543284\n",
      "train accuracy = 88.4%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Training rate\n",
    "alpha = 0.002\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Return the logistic function sigma(z) = 1/(1+exp(-z)).\"\"\"\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def cost(Y, Yhat):\n",
    "    \"\"\"Return the cost function for predictions Yhat of classifications Y.\"\"\"\n",
    "    return (- Y @ np.log(Yhat.T) - (1 - Y) @ np.log(1 - Yhat.T)) / m\n",
    "\n",
    "def accuracy(Y, Yhat):\n",
    "    \"\"\"Return measure of the accuracy with which Yhat predicts Y.\"\"\"\n",
    "    return 1 - np.mean(np.abs(Y - Yhat.round()))\n",
    "\n",
    "def model(X, w, b):\n",
    "    \"\"\"Apply the logistic model parameterized by w, b to features X.\"\"\"\n",
    "    z = w.T @ X + b\n",
    "    Yhat = sigmoid(z)\n",
    "    return z, Yhat\n",
    "\n",
    "\n",
    "def train(X, Y, max_it=1000):\n",
    "    \"\"\"Train the logistic regression algorithm on the data X classified as Y.\"\"\"\n",
    "\n",
    "    # Parameter vector, w, and constant term (bias), b.\n",
    "    # For random initialization, use the following:\n",
    "    #w, b = np.random.random((nx,1)) * 0.01, 0.01\n",
    "    # To initialize with zeros, use this line instead:\n",
    "    w, b = np.zeros((nx,1)), 0\n",
    "\n",
    "    def propagate(w, b):\n",
    "        \"\"\"Propagate the training by advancing w, b to reduce the cost, J.\"\"\"\n",
    "        z, Yhat = model(X, w, b)\n",
    "        w -= alpha / m * (X @ (Yhat - Y).T)\n",
    "        b -= alpha / m * np.sum(Yhat - Y)\n",
    "        J = np.squeeze(cost(Y, Yhat))\n",
    "        if not it % 100:\n",
    "            # Provide an update on the progress we have made so far.\n",
    "            print('{}: J = {}'.format(it, J))\n",
    "            print('train accuracy = {:g}%'.format(accuracy(Y, Yhat) * 100))\n",
    "        return w, b\n",
    "\n",
    "    # Train the model by iteratively improving w, b.\n",
    "    for it in range(max_it):\n",
    "        w, b = propagate(w, b)\n",
    "    return w, b\n",
    "\n",
    "import random\n",
    "\n",
    "n = len(file_names)\n",
    "train_indices = random.sample([i for i in range(n)], 3 * n // 4)\n",
    "test_indices = [i for i in range(n) if not i in train_indices]\n",
    "\n",
    "ds = h5py.File('./happyds.h5', 'r')\n",
    "\n",
    "ds_x = np.array([ds[\"x_train\"][i] for i in train_indices]).T / 100\n",
    "ds_y = np.array([ds[\"y_train\"][i] for i in train_indices])\n",
    "\n",
    "m = len(ds_y)\n",
    "\n",
    "# Dimension of the feature vector for each example.\n",
    "nx = ds_x.size // m\n",
    "# Packed feature vector and associated classification.\n",
    "X, Y = ds_x.reshape((nx, m)), ds_y.reshape((1, m))\n",
    "\n",
    "# Train the model\n",
    "w, b = train(X, Y, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[164, 31, 212, 4, 19, 235, 169, 193, 101, 50, 309, 220, 318, 37, 155, 17, 104, 239, 148, 270, 96, 288, 165, 170, 257, 116, 117, 89, 211, 228, 171, 102, 313, 253, 13, 214, 160, 178, 27, 158, 90]\n"
     ]
    }
   ],
   "source": [
    "# i = 18\n",
    "# ind = test_indices[i]\n",
    "# print(ind)\n",
    "\n",
    "# features = np.asarray(ds[\"x_train\"][ind], dtype='uint8')[:, :].T / 100\n",
    "# acc_y = ds[\"y_train\"][ind]\n",
    "\n",
    "# z, yhat = model(features.reshape(nx, 1), w, b)\n",
    "# print(acc_y, np.squeeze(yhat))\n",
    "\n",
    "works = []\n",
    "fails = []\n",
    "for inde in test_indices:\n",
    "    features = np.asarray(ds[\"x_train\"][inde], dtype='uint8')[:, :].T / 100\n",
    "    acc_y = ds[\"y_train\"][inde]\n",
    "\n",
    "    z, yhat = model(features.reshape(nx, 1), w, b)\n",
    "    \n",
    "    if (yhat < 0.5 and acc_y == 0):\n",
    "        works.append(inde)\n",
    "    elif (yhat > 0.5 and acc_y == 1):\n",
    "        works.append(inde)\n",
    "    else:\n",
    "        fails.append(inde)\n",
    "    \n",
    "print(fails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
